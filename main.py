import asyncio
import aiohttp
from datetime import datetime, timedelta
import random
import json
import logging

# å¯¼å…¥é…ç½®
from config import (
    API_URL,  # èŠ‚ç‚¹åˆ—è¡¨API
    PROFILE_API_URL,  # ä¸ªäººèµ„æ–™API
    EPOCH_EARNINGS_API_URL,  # æ”¶ç›ŠAPI
    TOKENS_CONFIG,
    WEBHOOK_URL, 
    PROXY_URL, 
    USE_PROXY, 
    INTERVAL, 
    TIME_OFFSET,
    ALWAYS_NOTIFY,
    APP_NAME,
    SHOW_DETAIL
)

# é…ç½®logging
def setup_logging():
    """é…ç½®æ—¥å¿—æ ¼å¼å’Œçº§åˆ«"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    return logging.getLogger(__name__)

# è·å–loggerå®ä¾‹
logger = setup_logging()

# å…¨å±€å­—å…¸ç”¨äºç¼“å­˜ä¸Šæ¬¡çš„epochæ”¶ç›Šæ•°æ®
previous_epoch_data_cache = {}

# æ–°å¢ï¼šéšæœºå»¶è¿Ÿå‡½æ•°
async def random_delay():
    """ç”Ÿæˆéšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆ10-20ç§’ï¼‰"""
    delay = random.uniform(10, 20)
    logger.info(f"ç­‰å¾… {delay:.2f} ç§’...")
    await asyncio.sleep(delay)

async def monitor_single_token(session, token_config, webhook_url, use_proxy, proxy_url):
    """ç›‘æ§å•ä¸ªtokençš„èŠ‚ç‚¹çŠ¶æ€å’Œepochæ”¶ç›Š"""
    try:
        logger.info(f"{'='*50}")
        logger.info(f"å¼€å§‹æ£€æŸ¥Token: {token_config['name']}")
        logger.info(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è·å–ç”¨æˆ·èµ„æ–™
        logger.info("æ­£åœ¨è·å–ç”¨æˆ·èµ„æ–™...")
        profile_data = await fetch_profile_data(session, token_config['token'])
        username = profile_data.get('username', 'æœªçŸ¥ç”¨æˆ·')
        logger.info(f"ç”¨æˆ·å: {username}")
        
        # è·å–å½“å‰Epochæ”¶ç›Šæ•°æ®
        logger.info("æ­£åœ¨è·å–Epochæ”¶ç›Šæ•°æ®...")
        current_epoch_data = await fetch_epoch_earnings(session, token_config['token'])
        
        # è·å–ä¸Šæ¬¡çš„Epochæ”¶ç›Šæ•°æ®
        previous_epoch_data = previous_epoch_data_cache.get(token_config['token'], [])
        
        # æ›´æ–°ç¼“å­˜ä¸ºå½“å‰æŸ¥è¯¢ç»“æœ
        previous_epoch_data_cache[token_config['token']] = current_epoch_data
        
        # æŒ‰epochåˆ†ç»„ç»Ÿè®¡æ•°æ®
        current_epoch_stats = group_epoch_data(current_epoch_data)
        previous_epoch_stats = group_epoch_data(previous_epoch_data)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¶ç›Šå˜åŒ–
        has_earnings_changed = should_send_epoch_notification(current_epoch_stats, previous_epoch_stats)
        if has_earnings_changed:
            logger.info("\næ£€æµ‹åˆ°Epochæ”¶ç›Šå˜åŒ–!")
        
        # è·å–èŠ‚ç‚¹æ•°æ®
        logger.info("æ­£åœ¨è·å–èŠ‚ç‚¹çŠ¶æ€...")
        result_data, total_uptime, online_count, offline_count = await fetch_nodes_data(
            session=session,
            api_url=API_URL,
            api_token=token_config['token']
        )
        
        if not result_data:
            logger.error(f"è·å–èŠ‚ç‚¹æ•°æ®å¤±è´¥ï¼Œè·³è¿‡æ­¤token: {token_config['name']}")
            return
            
        # åˆ¤æ–­æ˜¯å¦éœ€è¦å‘é€é€šçŸ¥
        should_notify = ALWAYS_NOTIFY or has_earnings_changed
        
        if should_notify:
            logger.info("æ„å»ºé€šçŸ¥æ¶ˆæ¯...")
            message = build_combined_message(
                token_name=token_config['name'],
                username=username,
                current_epoch_stats=current_epoch_stats,
                previous_epoch_stats=previous_epoch_stats,
                result_data=result_data,
                total_uptime=total_uptime,
                online_count=online_count,
                offline_count=offline_count,
                has_earnings_changed=has_earnings_changed
            )
            
            if message:
                logger.info(" å‘é€é€šçŸ¥æ¶ˆæ¯...")
                await send_message_async(webhook_url, message, use_proxy, proxy_url)
                logger.info("âœ… æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            logger.info("æ— å˜åŒ–ï¼Œè·³è¿‡é€šçŸ¥")
            
    except Exception as e:
        logger.error(f"âŒ ç›‘æ§Token {token_config['name']} æ—¶å‡ºé”™: {str(e)}")
        
    finally:
        logger.info(f"æ£€æŸ¥å®Œæˆ: {token_config['name']}")
        logger.info('='*50 + '\n')

def group_epoch_data(epoch_data):
    """æŒ‰epochåˆ†ç»„ç»Ÿè®¡æ•°æ®"""
    epoch_stats = {}
    for entry in epoch_data:
        epoch_stats[entry['epochName']] = {
            'totalPoints': entry['totalPoints'],
            'rewardPoints': entry['rewardPoints'],
            'referralPoints': entry['referralPoints'],
            'totalUptime': entry['totalUptime'],
            'modified': entry['modified']
        }
    return epoch_stats

def should_send_epoch_notification(current_stats, previous_stats):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦å‘é€epochæ”¶ç›Šé€šçŸ¥"""
    for epoch_name, current in current_stats.items():
        previous = previous_stats.get(epoch_name, {})
        if current.get('modified') != previous.get('modified'):
            return True
    return False

def build_combined_message(token_name, username, current_epoch_stats, previous_epoch_stats, 
                         result_data, total_uptime, online_count, offline_count, has_earnings_changed):
    """æ„å»ºåˆå¹¶åçš„ç”¨æˆ·ä¿¡æ¯æ¶ˆæ¯"""
    adjusted_time = datetime.now() + timedelta(hours=TIME_OFFSET)
    timestamp = adjusted_time.strftime('%Y-%m-%d %H:%M:%S')
    
    # åŸºç¡€ä¿¡æ¯
    message_lines = [
        f"ğŸ” ã€{APP_NAME} çŠ¶æ€æŠ¥å‘Šã€‘",
        f"â° æ—¶é—´: {timestamp}",
        f"ğŸ‘¤ è´¦æˆ·: {token_name} ({username})",
        
        # èŠ‚ç‚¹çŠ¶æ€æ‘˜è¦
        f"\nğŸ“¡ èŠ‚ç‚¹çŠ¶æ€:",
        f"  â€¢ æ€»èŠ‚ç‚¹æ•°: {len(result_data)}",
        f"  â€¢ åœ¨çº¿èŠ‚ç‚¹: {online_count}",
        f"  â€¢ ç¦»çº¿èŠ‚ç‚¹: {offline_count}",
        f"  â€¢ æ€»è¿è¡Œæ—¶é—´: {format_uptime(total_uptime)}"
    ]
    
    # åªåœ¨æœ‰æ”¶ç›Šå˜åŒ–æ—¶æ˜¾ç¤ºè¯¦ç»†çš„æ”¶ç›Šä¿¡æ¯
    if has_earnings_changed:
        message_lines.append("\nğŸ’° æ”¶ç›Šå˜åŒ–:")
        for epoch_name, stats in current_epoch_stats.items():
            # è®¡ç®—å½“å‰çš„æ€»ç§¯åˆ†
            current_epoch_points = stats['totalPoints'] + stats['rewardPoints']
            current_referral_points = stats['referralPoints']
            
            # è·å–ä¸Šä¸€æ¬¡çš„æ•°æ®
            previous_stats = previous_epoch_stats.get(epoch_name, {})
            previous_epoch_points = previous_stats.get('totalPoints', 0) + previous_stats.get('rewardPoints', 0)
            previous_referral_points = previous_stats.get('referralPoints', 0)
            
            # è®¡ç®—å¢é‡
            epoch_points_increase = current_epoch_points - previous_epoch_points
            referral_points_increase = current_referral_points - previous_referral_points
            
            if epoch_points_increase > 0 or referral_points_increase > 0:
                message_lines.extend([
                    f"\n{epoch_name}:",
                    f"  â€¢ æ€»ç§¯åˆ†: {current_epoch_points:,} (+{epoch_points_increase:,})",
                    f"  â€¢ æ¨èå¥–åŠ±: {current_referral_points:,} (+{referral_points_increase:,})",
                    f"  â€¢ è¿è¡Œæ—¶é—´: {format_uptime(stats['totalUptime'])}"
                ])
    else:
        message_lines.append("\nğŸ’¡ æ— æ”¶ç›Šå˜åŒ–")
    
    return "\n".join(message_lines)

def format_uptime(seconds):
    """æ ¼å¼åŒ–è¿è¡Œæ—¶é—´"""
    days = seconds // (24 * 3600)
    hours = (seconds % (24 * 3600)) // 3600
    minutes = (seconds % 3600) // 60
    
    parts = []
    if days > 0:
        parts.append(f"{days}å¤©")
    if hours > 0:
        parts.append(f"{hours}å°æ—¶")
    if minutes > 0:
        parts.append(f"{minutes}åˆ†é’Ÿ")
    
    return "".join(parts) if parts else "å°äº1åˆ†é’Ÿ"

def get_random_user_agent():
    """è·å–éšæœºUser-Agent"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0"
    ]
    return random.choice(user_agents)


async def send_message_async(webhook_url, message_content, use_proxy, proxy_url):
    headers = {'Content-Type': 'application/json'}
    
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    proxy = proxy_url if use_proxy else None
    async with aiohttp.ClientSession() as session:
        async with session.post(webhook_url, json=payload, headers=headers, proxy=proxy) as response:
            if response.status == 200:
                logger.info("Message sent successfully!")
            else:
                logger.error(f"Failed to send message: {response.status}, {await response.text()}")


async def fetch_nodes_data(session, api_url, api_token):
    """è·å–èŠ‚ç‚¹æ•°æ®"""
    headers = {
        "accept": "application/json, text/plain, */*",
        "accept-encoding": "gzip, deflate, br",
        "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
        "authorization": f"{api_token}",
        "content-type": "application/json",
        "user-agent": get_random_user_agent()
    }
    
    try:
        async with session.get(api_url, headers=headers, timeout=30, ssl=False) as response:
            logger.info(f"å“åº”çŠ¶æ€ç : {response.status}")
            
            if response.status == 403:
                logger.error(f"Tokenè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆ: {api_token}")
                return None, 0, 0, 0
            elif response.status == 200:
                data = await response.json()
                
                # è·å–åŸå§‹èŠ‚ç‚¹æ•°æ®
                raw_nodes = data.get('result', {}).get('data', {}).get('data', [])
                if SHOW_DETAIL:
                    logger.info(f"è·å–åˆ°çš„èŠ‚ç‚¹æ•°é‡: {len(raw_nodes)}")
                
                if raw_nodes:
                    try:
                        # æå–æ¯ä¸ªèŠ‚ç‚¹çš„å…³é”®ä¿¡æ¯å¹¶æ’åº
                        result_data = [extract_node_info(node) for node in raw_nodes]
                        
                        # æŒ‰çŠ¶æ€å’ŒIPåˆ†æ•°æ’åº
                        result_data.sort(key=lambda x: (-x['isConnected'], -x['ipScore'], x['ipAddress']))
                        
                        # æ‰“å°æ’åºåçš„èŠ‚ç‚¹ä¿¡æ¯
                        logger.info("\nèŠ‚ç‚¹çŠ¶æ€åˆ—è¡¨:")
                        for node in result_data:
                            log_message = (
                                f"{'ğŸŸ¢' if node['isConnected'] else 'ğŸ”´'} "
                                f"{node['ipAddress']}({node['ipScore']}åˆ†) "
                                f"{node['countryCode']} {node['multiplier']}x "
                                f"èŠ‚ç‚¹ {node['deviceId'][:8]}..."
                            )
                            logger.info(log_message)
                        
                        # æ£€æŸ¥åœ¨çº¿èŠ‚ç‚¹çš„é‡å¤IP
                        online_nodes = [
                            node for node in result_data 
                            if node['ipScore'] > 0 and node['isConnected']
                        ]
                        
                        ip_count = {}
                        duplicate_ips = set()
                        for node in online_nodes:
                            ip = node.get('ipAddress')
                            if ip:
                                ip_count[ip] = ip_count.get(ip, 0) + 1
                                if ip_count[ip] > 1:
                                    duplicate_ips.add(ip)
                        
                        # å¦‚æœå‘ç°é‡å¤IPï¼Œæ‰“å°è­¦å‘Š
                        if duplicate_ips:
                            logger.warning("\nâš ï¸ å‘ç°åœ¨çº¿èŠ‚ç‚¹é‡å¤IP:")
                            for ip in duplicate_ips:
                                duplicate_nodes = [
                                    node for node in online_nodes 
                                    if node.get('ipAddress') == ip
                                ]
                                logger.warning(f"IP {ip} è¢«ä»¥ä¸‹åœ¨çº¿è®¾å¤‡ä½¿ç”¨:")
                                for node in duplicate_nodes:
                                    logger.warning(f"  - è®¾å¤‡ID: {node.get('deviceId')}")
                        
                        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                        total_uptime = sum(node['totalUptime'] for node in result_data)
                        online_count = len(online_nodes)
                        offline_count = len(result_data) - online_count
                        
                        logger.info(f"èŠ‚ç‚¹ç»Ÿè®¡:")
                        logger.info(f"â€¢ æ€»åœ¨çº¿æ—¶é—´: {format_uptime(total_uptime)}")
                        logger.info(f"â€¢ åœ¨çº¿èŠ‚ç‚¹æ•°: {online_count}")
                        logger.info(f"â€¢ ç¦»çº¿èŠ‚ç‚¹æ•°: {offline_count}")
                        
                        return result_data, total_uptime, online_count, offline_count
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                        raise
                else:
                    logger.error("æœªè·å–åˆ°èŠ‚ç‚¹æ•°æ®")
                    return None, 0, 0, 0
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status}")
                return None, 0, 0, 0
                
    except Exception as e:
        logger.error(f"è·å–æ•°æ®å¤±è´¥: {str(e)}")
        return None, 0, 0, 0

async def fetch_profile_data(session, api_token):
    """è·å–ç”¨æˆ·èµ„æ–™æ•°æ®"""
    headers = {
        "accept": "application/json, text/plain, */*",
        "accept-encoding": "gzip, deflate, br",
        "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
        "authorization": f"{api_token}",
        "content-type": "application/json",
        "user-agent": get_random_user_agent()
    }
    
    try:
        async with session.get(PROFILE_API_URL, headers=headers) as response:
            if response.status == 200:
                data = await response.json()
                return data.get('result', {}).get('data', {})  # ç›´æ¥è¿”å› result.data å†…å®¹
            else:
                error_text = await response.text()
                raise Exception(f"è·å–ä¸ªäººèµ„æ–™å¤±è´¥: {response.status}, é”™è¯¯ä¿¡æ¯: {error_text}")
    except Exception as e:
        logger.error(f"è·å–ä¸ªäººèµ„æ–™å¤±è´¥: {str(e)}")
        raise

async def fetch_epoch_earnings(session, api_token):
    """è·å–epochæ”¶ç›Šæ•°æ®"""
    headers = {
        "accept": "application/json, text/plain, */*",
        "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
        "authorization": f"{api_token}",
        "content-type": "application/json",
        "user-agent": get_random_user_agent()
    }
    
    try:
        async with session.get(EPOCH_EARNINGS_API_URL, headers=headers) as response:
            if response.status == 200:
                data = await response.json()
                return data.get('result', {}).get('data', {}).get('data', [])
            else:
                error_text = await response.text()
                raise Exception(f"è·å–Epochæ”¶ç›Šæ•°æ®å¤±è´¥: {response.status}, é”™è¯¯ä¿¡æ¯: {error_text}")
    except Exception as e:
        logger.error(f"è·å–Epochæ”¶ç›Šæ•°æ®å¤±è´¥: {str(e)}")
        raise

def build_epoch_stats_message(epoch_data, username):
    """æ„å»ºepochç»Ÿè®¡æ¶ˆæ¯"""
    adjusted_time = datetime.now() + timedelta(hours=TIME_OFFSET)
    timestamp = adjusted_time.strftime('%Y-%m-%d %H:%M:%S')
    
    message_lines = [
        f"ğŸ† ã€{APP_NAME} Epochæ”¶ç›ŠæŠ¥å‘Šã€‘",
        f"â° æ—¶é—´: {timestamp}",
        f"ğŸ‘¤ è´¦æˆ·: {username}\n",
        f"ğŸ“Š æ”¶ç›Šç»Ÿè®¡:"
    ]
    
    for epoch_name, stats in epoch_data.items():
        total_points = stats['totalPoints'] + stats['rewardPoints']
        message_lines.extend([
            f"\n{epoch_name}:",
            f"  â€¢ æ€»ç§¯åˆ†: {total_points:,}",
            f"  â€¢ æ¨èå¥–åŠ±: {stats['referralPoints']:,}",
            f"  â€¢ è¿è¡Œæ—¶é—´: {format_uptime(stats['totalUptime'])}"
        ])
    
    return "\n".join(message_lines)

async def monitor_nodes(interval, webhook_url, use_proxy, proxy_url, always_notify=False):
    """ç›‘æ§èŠ‚ç‚¹çŠ¶æ€"""
    iteration = 1
    while True:
        try:
            print(f"\nå¼€å§‹ç¬¬ {iteration} è½®æ£€æŸ¥...")
            
            # ä¸²è¡Œæ‰§è¡Œæ¯ä¸ªtokençš„ç›‘æ§
            for token_config in TOKENS_CONFIG:
                await monitor_token_with_session(
                    token_config=token_config,
                    webhook_url=webhook_url,
                    use_proxy=use_proxy,
                    proxy_url=proxy_url
                )
                # åœ¨æ¯ä¸ªtokenæ£€æŸ¥ä¹‹é—´æ·»åŠ éšæœºå»¶è¿Ÿ
                await random_delay()
                
            print(f"ç¬¬ {iteration} è½®æ£€æŸ¥å®Œæˆ\n")
            iteration += 1
            
        except Exception as e:
            print(f"ç›‘æ§è¿‡ç¨‹å‡ºé”™: {str(e)}")
            await asyncio.sleep(5)
            continue
            
        await asyncio.sleep(interval)

async def monitor_token_with_session(token_config, webhook_url, use_proxy, proxy_url):
    """ä¸ºæ¯ä¸ªtokenåˆ›å»ºç‹¬ç«‹çš„sessionè¿›è¡Œç›‘æ§"""
    timeout = aiohttp.ClientTimeout(total=60)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        await monitor_single_token(
            session=session,
            token_config=token_config,
            webhook_url=webhook_url,
            use_proxy=use_proxy,
            proxy_url=proxy_url
        )

def extract_node_info(node):
    """æå–èŠ‚ç‚¹çš„å…³é”®ä¿¡æ¯"""
    return {
        # åŸºç¡€ä¿¡æ¯
        'deviceId': node.get('deviceId'),
        'name': node.get('name'),
        'type': node.get('type'),
        
        # çŠ¶æ€ä¿¡æ¯
        'ipAddress': node.get('ipAddress'),
        'ipScore': node.get('ipScore', 0),
        'isConnected': node.get('isConnected', False),
        'totalUptime': node.get('totalUptime', 0),
        'lastConnectedAt': node.get('lastConnectedAt'),
        
        # åœ°ç†ä½ç½®
        'countryCode': node.get('countryCode'),
        
        # æ€§èƒ½æŒ‡æ ‡
        'multiplier': node.get('multiplier', 1),
        'totalPoints': node.get('totalPoints', 0)
    }

if __name__ == "__main__":
    asyncio.run(monitor_nodes(
        interval=INTERVAL,
        webhook_url=WEBHOOK_URL,
        use_proxy=USE_PROXY,
        proxy_url=PROXY_URL,
        always_notify=ALWAYS_NOTIFY  # æ·»åŠ è¿™ä¸ªå‚æ•°æ¥å¯ç”¨å§‹ç»ˆé€šçŸ¥
    ))

